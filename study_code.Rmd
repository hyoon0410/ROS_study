---
title: "study code"
author: "heedong"
date: "4/18/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library, results=FALSE, message=FALSE, warning=FALSE}
# load library
library('tidyverse')
library('here')
library("brms")
library("rstanarm")

```

## Chapter 1

#get dataset
```{r}
hibbs <- here("ROS-Examples-master", "ElectionsEconomy", "data", "hibbs.dat")

hibbs <- read.table(hibbs, header = TRUE)

```

#create model
```{r}
m1 <- brm(
  vote ~ growth, 
  data = hibbs
)
```

#plot the data and add fitted line (Figure 1.1)
```{r}
# abline(fixef(m1), col="gray")
M1 <-fixef(m1)
plot(
  hibbs$growth, 
  hibbs$vote, 
  xlab="Average recent growth in personal income",
  ylab="Incumbent party's vote share"
)

abline(a = M1[1,1], b = M1[2,1], col = "gray")
abline(coef = c(M1[1,1], M1[2,1]), col ="red")
abline(coef = M1[,1], col ="blue")
```

#1.6 
#examples of how the models can be fit to the data 
```{r}
fit <- brm(y ~ x, data=mydata)

fit <- stan_glm(y ~ x, data=mydata)

fit <- lm(y ~ x, data=mydata)

fit <- stan_glm(y ~ x, data=mydata, algorithm="optimizing")
```

##Chapter 2
#get dataset
```{r}
health <- here("ROS-Examples-master", "HealthExpenditure", "data", "healthdata.txt")

health <- read.table(health, header = TRUE)
```

#draw a scatter plot (Figure 2.4)
```{r}
country <- rownames(health)

plot(health$spending, health$lifespan, type="n")

text(health$spending, health$lifespan, country)
```

#try model fitting 
```{r}
model2 <- brm(
  lifespan ~ spending, 
  data = health
)

# abline(fixef(m2), col="gray")
M2 <-fixef(model2)
# plot(
#   health$lifespan, 
#   health$spending, 
#   xlab="Average recent growth in personal income",
#   ylab="Incumbent party's vote share"
# )

abline(a = M2[1,1], b = M2[2,1], col = "gray")
abline(coef = c(M2[1,1], M2[2,1]), col ="red")
abline(coef = M2[,1], col ="blue")

```

##Chapter 3
```{r}
dnorm(0.5, 0.5, 0.2)/2e5

dbinom(1e5, 2e5, 0.5)

dbinom(1e5, 2e5, 0.49)
```


```{r}
x = runif(100, -1, 1)

n = 100

x = runif(n, -1, 1)

y = 0.2 + 0.3*x + rnorm(n, 0, 0.5)

plot(x, y)

y = 0.2 + 0.3*x + rnorm(n, 0, 0.1)

lot(x, y)


df = data.frame(x = x, y = y)

lm1 = lm(y~x, data = df)

summary(lm1)
```

##chapter 4 
```{r}
estimate <- 40/100# y/n

se <- sqrt(estimate*(1-estimate)/n)
se
interval_95 <- estimate + qnorm(c(0.025, 0.975))*se
interval_95
```

##chapter 5

#5.1
```{r}
n_girls <- rbinom(1, 400, 0.488)

print(n_girls)

n_sims <- 1000
n_girls <- rep(NA, n_sims) #replicates values in x 
for (s in 1:n_sims){
 n_girls[s] <- rbinom(1, 400, 0.488)
}
hist(n_girls) #computes a histogram

n_girls <- rbinom(n_sims, 400, 0.488)

birth_type <- sample(c("fraternal twin","identical twin","single birth"),
 size=400, replace=TRUE, prob=c(1/125, 1/300, 1 - 1/125 - 1/300))
girls <- rep(NA, 400)

#400 birth events 
for (i in 1:400){ 
 if (birth_type[i]=="single birth") {
  girls[i] <- rbinom(1, 1, 0.488)
 } else if (birth_type[i]=="identical twin") {
  girls[i] <- 2*rbinom(1, 1, 0.495)
 } else if (birth_type[i]=="fraternal twin") {
  girls[i] <- rbinom(1, 2, 0.495)
 }
}
n_girls <- sum(girls)

girls <- ifelse(birth_type=="single birth", rbinom(400, 1, 0.488),
 ifelse(birth_type=="identical twin", 2*rbinom(400, 1, 0.495),
 rbinom(400, 2, 0.495)))

girls <- ifelse(birth_type=="single birth", rbinom(400, 1, 0.488),
 ifelse(birth_type=="identical twin", 2*rbinom(400, 1, 0.495),
 rbinom(400, 2, 0.495)))

```

#5.2
```{r}
n_sims <- 1000
   y1 <- rnorm(n_sims, 3, 0.5)
   y2 <- exp(y1)
   y3 <- rbinom(n_sims, 20, 0.6)
   y4 <- rpois(n_sims, 5)
   par(mfrow=c(2,2))
   hist(y1, breaks=seq(floor(min(y1)), max(y1) + 0.2, 0.2),
     main="1000 draws from a normal dist. with mean 3, sd 0.5")
   hist(y2, breaks=seq(0, max(y2) + 5, 5),
     main="1000 draws from the corresponding lognormal dist.")
   hist(y3, breaks=seq(-0.5, 20.5, 1),
     main="1000 draws from the binomial dist. with 20 tries, probability 0.6")
   hist(y4, breaks=seq(-0.5, max(y4) + 1, 1),
main="1000 draws from the Poisson dist. with mean 5")
```

#5.4
```{r}
earning_dir <- here("ROS-Examples-master", "Earnings", "data", "earnings.csv")

earnings <- read.csv(earning_dir, "earnings.csv", col_names = TRUE)
earn <- earnings$earn
male <- earnings$male
ratio <- median(earn[male==0]) / median(earn[male==1])

n <- nrow(earnings)
   boot <- sample(n, replace=TRUE)
   earn_boot <- earn[boot]
   male_boot <- male[boot]
   ratio_boot <- median(earn_boot[male_boot==0]) / median(earn_boot[male_boot==1])
   
boot_ratio <- function(data){
     n <- nrow(data)
     boot <- sample(n, replace=TRUE)
     earn_boot <- data$earn[boot]
     male_boot <- data$male[boot]
     median(earn_boot[male_boot==0]) / median(earn_boot[male_boot==1])
}


n_sims <- 10000
output <- replicate(n_sims, boot_ratio(data=earnings))
   
hist(output)
print(sd(output))
   

```

##chapter 6
#6.2
```{r}
x <- 1:20
   n <- length(x)
   a <- 0.2
   b <- 0.3
   sigma <- 0.5
   y <- a + b*x + sigma*rnorm(n)
   

fake <- data.frame(x, y)

fit_1 <- brm(y ~ x, data=fake)

print(fit_1, digits=2)

plot(fake$x, fake$y, main="Data and fitted regression line")

#a_hat <- coef(fit_1)[1]
a_hat = 0.49
#b_hat <- coef(fit_1)[2]
b_hat = 0.27
  
abline(a_hat, b_hat)


   x_bar <- mean(fake$x)
   text(x_bar, a_hat + b_hat*x_bar,
     paste("y =", round(a_hat, 2), "+", round(b_hat, 2), "* x"), adj=0)
```

#6.5
```{r}
n <- 1000
   true_ability <- rnorm(n, 50, 10)
   noise_1 <- rnorm(n, 0, 10)
   noise_2 <- rnorm(n, 0, 10)
   midterm <- true_ability + noise_1
   final <- true_ability + noise_2
   exams <- data.frame(midterm, final)
   
fit_1 <- brm(final ~ midterm, data=exams)
   plot(midterm, final, xlab="Midterm exam score", ylab="Final exam score")
   #abline(coef(fit_1))
   abline(fixef(fit_1)[,1]) #brm에서는 결과를 뽑는 방법이 다르기 때문에 다른 function 사용해야 함. 
```


##chapter 7 

#7.1
```{r}
hibbs <- here("ROS-Examples-master", "ElectionsEconomy", "data", "hibbs.dat")

hibbs <- read.table(hibbs, header = TRUE)

plot(hibbs$growth, hibbs$vote, xlab="Economic growth",
 ylab="Incumbent party's vote share")
```

#model fitting
```{r}
m1 <- brm(vote ~ growth, 
          data=hibbs)

print(m1)

M1 <- fixef(m1)

abline(a = M1[1,1], b = M1[2,1], col = "red")


1 - pnorm(50, 52.3, 3.9)
```

#7.2
#Step 1: Creating the pretend world
```{r}
a <- 46.3
b <- 3.0
sigma <- 3.9
x <- hibbs$growth
n <- length(x)
```

#Step 2: Simulating fake data
```{r}
y <- a + b*x + rnorm(n, 0, sigma)
   fake <- data.frame(x, y)
```

#Step 3: Fitting the model and comparing fitted to assumed values
```{r}
fit <- brm(y ~ x, data=fake)
   print(fit)
   
b_hat <- fixef(fit)["x"]
 b_se <- se(fit)["x"]
 
cover_68 <- abs(b - b_hat) < b_se
   cover_95 <- abs(b - b_hat) < 2*b_se
   cat(paste("68% coverage: ", cover_68, "\n"))
   cat(paste("95% coverage: ", cover_95, "\n")) 
```

#Step 4: Embedding the simulation in a loop
```{r}
n_fake <- 1000
cover_68 <- rep(NA, n_fake)
cover_95 <- rep(NA, n_fake)
for (s in 1:n_fake){
  y <- a + b*x + rnorm(n, 0, sigma)
  fake <- data.frame(x, y)
  fit <- stan_glm(y ~ x, data=fake, refresh=0)
  b_hat <- coef(fit)["x"]
  b_se <- se(fit)["x"]
  cover_68[s] <- abs(b - b_hat) < b_se
  cover_95[s] <- abs(b - b_hat) < 2*b_se
}
cat(paste("68% coverage: ", mean(cover_68), "\n"))
cat(paste("95% coverage: ", mean(cover_95), "\n"))
# suppress output on console
```

#7.3
```{r}
n_0 <- 20
y_0 <- rnorm(n_0, 2.0, 5.0)
fake_0 <- data.frame(y_0)
print(y_0)

mean(y_0)
se <- sd(y_0)/sqrt(n_0)



fit_0 <- brm(y_0 ~ 1, data=fake_0#,
#prior_intercept=NULL,
#prior=NULL,
#prior_aux=NULL
)
print(fit_0)


```

```{r}
n_1 <- 30
y_1 <- rnorm(n_1, 8.0, 5.0)

diff <- mean(y_1) - mean(y_0)
se_0 <- sd(y_0)/sqrt(n_0)
se_1 <- sd(y_1)/sqrt(n_1)
se <- sqrt(se_0^2 + se_1^2)
```


#chapter 8

```{r}
rss <- function(x, y, a, b){     # x and y are vectors, a and b are scalars
     resid <- y - (a + b*x)
     return(sum(resid^2))
}
```

#chapter 9

#model from 7.1 
```{r}
hibbs <- here("ROS-Examples-master", "ElectionsEconomy", "data", "hibbs.dat")

hibbs <- read.table(hibbs, header = TRUE)

plot(hibbs$growth, hibbs$vote, xlab="Economic growth",
 ylab="Incumbent party's vote share")

earnings <- here("ROS-Examples-master", "Earnings", "data", "earnings.csv")
earnings <- read.csv(earnings, header = TRUE)

```

#model fitting
```{r}
m1 <- brm(vote ~ growth, 
          data=hibbs)

print(m1)

M1 <- fixef(m1)

abline(a = M1[1,1], b = M1[2,1], col = "red")

# parameter vector (a, b, σ): the intercept, the slope, and the residual standard deviation of the regression
1 - pnorm(50, 52.3, 3.9)
```

#9.1 Propagating uncertainty in inference using posterior simulations
```{r}
sims <- as.matrix(M1)

#The mad sd is the scaled median absolute deviation (see Section 5.3) of the posterior simulations, which we use to summarize uncertainty
Median <- apply(sims, 2, median)
MAD_SD <- apply(sims, 2, mad)
print(cbind(Median, MAD_SD))

a <- sims[,1]
b <- sims[,2]
z <- a/b
print(c(median(z), mad(z)))
```

#Point prediction using predict
```{r}
#여기서 m1 은 모델을 fixef 한 값 

new <- data.frame(growth=2.0)

#point prediction
y_point_pred <- predict(m1, newdata=new)

a_hat <- fixef(m1)[1]
b_hat <- fixef(m1)[2]
y_point_pred <- a_hat + b_hat*new
```

#Linear predictor with uncertainty using posterior_linpred or posterior_epred
```{r}
y_linpred <- posterior_linpred(m1, newdata=new)

sims <- as.matrix(m1)
a <- sims[,1]
b <- sims[,2]
y_linpred <- a + b*new
```

#Predictive distribution for a new observation using posterior_predict
```{r}
y_pred <- posterior_predict(m1, newdata=new)


n_sims <- nrow(sims)
sigma <- sims[,3]
y_pred <- as.numeric(a + b*new) + rnorm(n_sims, 0, sigma)

hist(y_pred)

y_pred_median <- median(y_pred)
y_pred_mad <- mad(y_pred)
win_prob <- mean(y_pred > 50)
cat("Predicted Clinton percentage of 2-party vote: ", round(y_pred_median,1),
     ", with s.e. ", round(y_pred_mad, 1), "\nPr (Clinton win) = ", round(win_prob, 2),
     sep="")
```

#Prediction given a range of input values
```{r}
new_grid <- data.frame(growth=seq(-2.0, 4.0, 0.5))
y_point_pred_grid <- predict(m1, newdata=new_grid)
y_linpred_grid <- posterior_linpred(m1, newdata=new_grid)
y_pred_grid <- posterior_predict(m1, newdata=new_grid)
```

#Propagating uncertainty
```{r}
x_new <- rnorm(n_sims, 2.0, 0.3)
y_pred <- rnorm(n_sims, a + b*x_new, sigma)
```

#Simulating uncertainty for the linear predictor and new observations
```{r}
fit_1 <- brm(weight ~ height, data=earnings)
print(fit_1)


earnings$c_height <- earnings$height - 66
fit_2 <- brm(weight ~ c_height, data=earnings)
print(fit_2)


new <- data.frame(c_height=4.0)

y_point_pred_2 <- predict(fit_2, newdata=new)

y_linpred_2 <- posterior_linpred(fit_2, newdata=new)

y_postpred_2 <- posterior_predict(fit_2, newdata=new)
```

#Bayesian information aggregation
```{r}
theta_hat_prior <- 0.524
se_prior <- 0.041
n <- 400
y <- 190
theta_hat_data <- y/n
se_data <- sqrt((y/n)*(1-y/n)/n)
theta_hat_bayes <- (theta_hat_prior/se_prior^2 + theta_hat_data/se_data^2) /
  (1/se_prior^2 + 1/se_data^2)
se_bayes <- sqrt(1/(1/se_prior^2 + 1/se_data^2))
```

#Uniform prior distribution
```{r}
M3 <- stan_glm(vote ~ growth, data=hibbs, prior_intercept=NULL, prior=NULL, prior_aux=NULL)
M3
sims <- as.data.frame(M3)
a <- sims[,1]
b <- sims[,2]
plot(a, b)
```

#Default prior distribution
```{r}
sd_x <- sd(hibbs$growth)
sd_y <- sd(hibbs$vote)
mean_y <- mean(hibbs$vote)
M1a <- stan_glm(vote ~ growth, data=hibbs, prior=normal(0, 2.5*sd_y/sd_x), prior_intercept=normal(mean_y, 2.5*sd_y), prior_aux=exponential(1/sd_y))
```

#Weakly informative prior distribution based on subject-matter knowledge
```{r}
M4 <- stan_glm(vote ~ growth, data=hibbs,
     prior=normal(5, 5), prior_intercept=normal(50, 10))
M4
```

#Example where an informative prior makes a difference: Beauty and sex ratio
#get data 
```{r}
# sexratio <- here("ROS-Examples-master", "SexRatio", "data", "sexratio.rda")
# 
# sexratio<- read.table(sexratio, header = TRUE)

x <- seq(-2,2,1)
y <- c(50, 44, 50, 47, 56)
sexratio <- data.frame(x, y)

fit_post0 <- stan_glm(y ~ x, data=sexratio)

fit_post <- stan_glm(y ~ x, data=sexratio,
     prior=normal(0, 0.2), prior_intercept=normal(48.8, 0.5))
print(fit_post)
sims0 <- as.data.frame(fit_post0)
simsS <- as.data.frame(fit_post)
a <- simsS[,1]
b <- simsS[,2]
a0 <-sims0[,1]
b0 <- sims0[,2]
plot(a, b)
plot(a0, b0)

```

##Chapter 10 
#10.1 Adding predictors to a model
#Single predictor (binary)
```{r}
#call data
kidiq <- here("ROS-Examples-master", "KidIQ", "data", "kidiq.csv")

kidiq<- read.csv(kidiq, header = TRUE)

#binary predictor
fit_kid <- stan_glm(kid_score ~ mom_hs, data=kidiq)
fit_kid
```

#A single continuous predictor
```{r}
fit_kid_cont <- stan_glm(kid_score ~ mom_iq, data=kidiq)
fit_kid_cont
```

#Including both predictors
```{r}
fit_3 <- stan_glm(kid_score ~ mom_hs + mom_iq, data=kidiq)
print(fit_3)

```

#10.3 Interactions
```{r}
fit_4 <- stan_glm(kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq, data=kidiq)
print(fit_4)
```

#10.4 Indicator variables 
```{r}
#call data
earnings <- here("ROS-Examples-master", "Earnings", "data", "earnings.csv")

earnings<- read.csv(earnings, header = TRUE)

fit_1 <- stan_glm(weight ~ height, data=earnings)
print(fit_1)


coefs_1 <- coef(fit_1)
predicted_1 <- coefs_1[1] + coefs_1[2]*66

new <- data.frame(height=66)
pred <- posterior_predict(fit_1, newdata=new)

cat("Predicted weight for a 66-inch-tall person is", round(mean(pred)),
       "pounds with a sd of", round(sd(pred)), "\n")
```

#centering a predictor
```{r}
earnings$c_height <- earnings$height - 66
fit_2 <- stan_glm(weight ~ c_height, data=earnings)
print(fit_2)
```

#Including a binary variable in a regression
```{r}
fit_3 <- stan_glm(weight ~ c_height + male, data=earnings)

#woman data
coefs_3 <- coef(fit_3)
predicted <- coefs_3[1] + coefs_3[2]*4.0 + coefs_3[3]*0

new <- data.frame(c_height=4.0, male=0)
pred <- posterior_predict(fit_3, newdata=new)
cat("Predicted weight for a 70-inch-tall woman is", round(mean(pred)),
"pounds with a sd of", round(sd(pred)), "\n")

```

#Using indicator variables for multiple levels of a categorical predictor
```{r}
fit_4 <- stan_glm(weight ~ c_height + male + factor(ethnicity), data=earnings)

print(fit_4)
```

#Changing the baseline factor level
```{r}
earnings$eth <- factor(earnings$ethnicity,
     levels=c("White", "Black", "Hispanic", "Other"))

fit_5 <- stan_glm(weight ~ c_height + male + eth, data=earnings)
print(fit_5)

```

#10.5 
#paired design
```{r}
# earnings <- here("ROS-Examples-master", "Earnings", "data", "earnings.csv")
# 
# earnings<- read.csv(earnings, header = TRUE)

```

#Chapter 12 
#12.1 Linear transformations
```{r}
earnings <- here("ROS-Examples-master", "Earnings", "data", "earnings.csv")

earnings<- read.csv(earnings, header = TRUE)

fit1 <- stan_glm(earn ~ height, data=earnings)
fit2 <- brm(earn ~ height, data=earnings)

fit1
fit2
```

#Centering by subtracting the mean of the data 
```{r}
kidiq <- here("ROS-Examples-master", "KidIQ", "data", "kidiq.csv")

kidiq<- read.csv(kidiq, header = TRUE)

kidiq$c_mom_hs <- kidiq$mom_hs - mean(kidiq$mom_hs)

kidiq$c_mom_iq <- kidiq$mom_iq - mean(kidiq$mom_iq)
```

#Standardizing by subtracting the mean and dividing by 2 standard deviations
```{r}
kidiq$z_mom_hs <- (kidiq$mom_hs - mean(kidiq$mom_hs))/(2*sd(kidiq$mom_hs))

kidiq$z_mom_iq <- (kidiq$mom_iq - mean(kidiq$mom_iq))/(2*sd(kidiq$mom_iq))
```

#Earnings and height example 
```{r}

logmodel_1 <- stan_glm(log(earn) ~ height, data=earnings, subset=earn>0)
print(logmodel_1)

yrep_1 <- posterior_predict(fit_1)

n_sims <- nrow(yrep_1)

subset <- sample(n_sims, 100)
library("bayesplot")
ppc_dens_overlay(earnings$earn, yrep_1[subset,])
```

#Building a regression model on the log scale
```{r}
logmodel_2 <- stan_glm(log(earn) ~ height + male, data=earnings, subset=earn>0)

```

#Including an interaction
```{r}
logmodel_3 <- stan_glm(log(earn) ~ height + male + height:male,
     data=earnings, subset=earn>0)
```

#Linear transformation to make coefficients more interpretable.
```{r}
earnings$z_height <- (earnings$height - mean(earnings$height))/sd(earnings$height)
```

#Example: predicting the yields of mesquite bushes (leave-one-out (LOO) cross validation
```{r}
mesquite <- here("ROS-Examples-master", "Mesquite", "data", "mesquite.dat")

mesquite<- read.table(mesquite, header = TRUE)

fit_1 <- stan_glm(formula = weight ~ diam1 + diam2 + canopy_height +
                     total_height + density + group, data=mesquite)

(loo_1 <- loo(fit_1))

fit_2 <- stan_glm(formula = log(weight) ~ log(diam1) + log(diam2) + log(canopy_height) + log(total_height) + log(density) + group, data=mesquite)

(loo_2 <- loo(fit_2))

canopy_volume = mesquite$diam1 * mesquite$diam2 * mesquite$canopy_height

fit_3 <- stan_glm(log(weight) ~ log(canopy_volume), data=mesquite)

loo_3 <- loo(fit_3)
loo_compare(loo_2, loo_3)
```


```{r}

```

```{r}

```

```{r}

```

```{r}

```

